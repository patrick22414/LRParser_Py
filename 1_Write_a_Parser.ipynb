{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Parser\n",
    "\n",
    "## What is\n",
    "\n",
    "> In computer science, LR parsers are a type of bottom-up parser that analyse deterministic context-free languages in linear time. – Wikipedia\n",
    "\n",
    "Yeah, that makes no sense to you just as it is to me.\n",
    "\n",
    "So in this notebook, I will try to explain what an LR parser is in my own understanding, starting from smaller concepts and working our way up to the somewhat complex idea of the parser as a whole (much like the bottom-up approach of LR parser itself). And we will write an LR parser in Python together.\n",
    "\n",
    "But I must first admit, that much of my own understanding of LR parser comes from the Wikipedia cited above, and I have zero academic reference whatsoever. There is no guarantee to the theoretical rigour of any of the content below. This is, again just my personal train of thought derailed in front of you.\n",
    "\n",
    "Let's begin.\n",
    "\n",
    "Oh before we begin, let's just do some Python imports here, since there's really no better place to put them in this notebook. There is no need to worry if you don't know some of the imported stuff. It's only relevent to our implementation, mostly typing, and has nothing to do with the notion of LR parser itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from pprint import pprint\n",
    "from string import ascii_uppercase\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "from ordered_set import OrderedSet\n",
    "from typing_extensions import Self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token\n",
    "\n",
    "Tokens are words, or individual components found in your source code. The following Python source code:\n",
    "```python\n",
    "def my_function():\n",
    "    pass\n",
    "```\n",
    "corresponds to 11 tokens:\n",
    "```\n",
    "'def' 'my_function' '(' ')' ':' 'NEWLINE' 'INDENT' 'pass' 'NEWLINE' 'DEDENT' 'END'\n",
    "```\n",
    "\n",
    "Here I used single quotes to show you each token, but this is no standard and is simply for clarity. Notice some tokens are not easily visible from the source code, such as NEWLINE, INDENT, DEDENT, and END.\n",
    "\n",
    "You can verify this using the `tokenize` module from Python standard library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME 'def'\n",
      "NAME 'my_function'\n",
      "OP '('\n",
      "OP ')'\n",
      "OP ':'\n",
      "NEWLINE '\\n'\n",
      "INDENT '    '\n",
      "NAME 'pass'\n",
      "NEWLINE '\\n'\n",
      "DEDENT ''\n",
      "ENDMARKER ''\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from tokenize import generate_tokens, tok_name\n",
    "\n",
    "source_code = \"\"\"\\\n",
    "def my_function():\n",
    "    pass\n",
    "\"\"\"\n",
    "for token in generate_tokens(io.StringIO(source_code).readline):\n",
    "    print(tok_name[token.type], repr(token.string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job of an LR parser (or any parser) is to take a stream of tokens as input, and output an Abstract Syntax Tree.\n",
    "```python\n",
    "def lr_parser(tokens: List[Token]) -> ASTree:\n",
    "```\n",
    "\n",
    "Well, OK, when I said tokens are words, I lied a bit. Here we are going to slightly overload the concept of a token. Tokens are not just the words you saw above, but also larger structures they make up of. So the source code `1 + 1` is not just three tokens, but can also be seen as 1 `Add` token, which is in turn an expression, or `Expr` token. These are shown in the following tree structure, known as an Abstract Syntax Tree.\n",
    "```\n",
    "Expr\n",
    "└─ Add\n",
    "   ├─ '1'\n",
    "   ├─ '+'\n",
    "   └─ '1'\n",
    "```\n",
    "\n",
    "You can probably feel that '1' and Expr are quite different: '1' is very concrete and Expr is more abstract. In fact, we call the first type of tokens **Terminals**, and the second type **Non-Terminals**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminals\n",
    "Terminal tokens are single tokens that you can find from the token stream. They are called Terminals because they will become the leaf nodes of your tree. I will denote Terminal tokens with single quotes. The following are five Terminals:\n",
    "```python\n",
    "'def'   # the keyword return\n",
    "'3.14'  # the number 3.14\n",
    "'+'     # the plus sign\n",
    "'('     # the left parenthesis\n",
    "'\"abc\"' # the string literal \"abc\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Terminals\n",
    "Non-Terminal tokens are the non-leaf nodes in your tree. I will denote Non-Terminal tokens with a word starting with an uppercase letter. These are four Non-Terminals:\n",
    "```python\n",
    "Stmt # a statement\n",
    "Expr # an expression\n",
    "Add  # an add expression, such as 1 + 2\n",
    "Atom # an atomic expression, such as an identifier or a number\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen the concept of tokens, let's write them in Python.\n",
    "\n",
    "Here I use `NamedTuple` to make a token immutable and hashable, which will come in handy later. I also override the `__repr__` functions to make them look nicer when printed, but don't worry about these.\n",
    "\n",
    "The `make_token` function helps us to create a token by giving it a string in the notation I used above: either in single quotes or starting with an upper case letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term('3.14') Term('+') NTerm(Expr)\n"
     ]
    }
   ],
   "source": [
    "class Token(NamedTuple):\n",
    "    name: str\n",
    "\n",
    "\n",
    "class Term(Token):  # Terminals\n",
    "    def __repr__(self) -> str:\n",
    "        return \"Term(\" + self.name + \")\"\n",
    "\n",
    "\n",
    "class NTerm(Token):  # Non-Terminals\n",
    "    def __repr__(self) -> str:\n",
    "        return \"NTerm(\" + self.name + \")\"\n",
    "\n",
    "\n",
    "def make_token(token: str) -> Token:\n",
    "    if token[0] in ascii_uppercase:\n",
    "        return NTerm(token)\n",
    "    elif token[0] == \"'\":\n",
    "        return Term(token)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "print(make_token(\"'3.14'\"), make_token(\"'+'\"), make_token(\"Expr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule\n",
    "\n",
    "**Rules** are where we define our syntax, or grammar. A Rule starts with a Non-Terminal token, followed by a sequence of Terminal and Non-Terminal tokens.\n",
    "\n",
    "For example, we can make a rule that an Atom is made of either a '0' or a '1'.\n",
    "```\n",
    "Atom <= '0'\n",
    "Atom <= '1'\n",
    "```\n",
    "\n",
    "We’ll call the Non-Terminal token to the left of `<=` the Left-Hand-Side of a Rule, or LHS. Similarly, the sequence of mixed tokens to the right of `<=`  is called the Right-Hand-Side of a Rule, or RHS.\n",
    "\n",
    "Rules can be recursive. In fact, one of the major advantage of LR parsers is the ability to handle _Left Recursion_, but I won't go into detail here.\n",
    "\n",
    "For example, an Expr can be made of another Expr, a plus sign, and an Atom.\n",
    "```\n",
    "Expr <= Expr '+' Atom\n",
    "```\n",
    "\n",
    "Again, let's write this in Python. And we'll also write a `make_rule` function to help us create a rule from a grammar string like above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule: NTerm(Expr) <= NTerm(Expr) Term('+') NTerm(Atom)\n"
     ]
    }
   ],
   "source": [
    "class Rule(NamedTuple):\n",
    "    left: Token\n",
    "    right: tuple[Token]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"Rule: \" + str(self.left) + \" <= \" + \" \".join(map(str, self.right))\n",
    "\n",
    "\n",
    "def make_rule(grammar: str) -> Rule:\n",
    "    left, right = grammar.split(\" <= \")\n",
    "\n",
    "    left = make_token(left)\n",
    "    assert isinstance(left, NTerm)\n",
    "\n",
    "    right = tuple(make_token(token) for token in right.split())\n",
    "\n",
    "    return Rule(left, right)\n",
    "\n",
    "print(make_rule(\"Expr <= Expr '+' Atom\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Items and Item Sets\n",
    "\n",
    "I believe so far things have been straightforward. But it's gonna start to get a bit confusing from here.\n",
    "\n",
    "A Rule can generate many **LR(0) Items**, or simply **Items**, by placing a \"dot\" at different positions of the RHS of the Rule. For clarity, I will represent the \"dot\" with the exponent or XOR sign `^`.\n",
    "\n",
    "The Rule above corresponds to 4 Items:\n",
    "```\n",
    "Expr <= ^ Expr '+' Atom\n",
    "Expr <= Expr ^ '+' Atom\n",
    "Expr <= Expr '+' ^ Atom\n",
    "Expr <= Expr '+' Atom ^\n",
    "```\n",
    "\n",
    "Imagine the \"dot\" is a cursor that moves rightwards as you parse through the Rule. So for a Rule with `n` tokens on its RHS, there are `n+1` positions for the \"dot\", and therefore `n+1` Items.\n",
    "\n",
    "Once you understood what an Item is, an Item Set is much simpler to understand. It is simply a set of Items. Note that these Items can be made from different rules.\n",
    "\n",
    "When implementing this in Python, we'll need the ability to move the \"dot\" to the right by 1 token, and to know what's immediately after the \"dot\", so we write these as two methods of the Item class. The ItemSet class subclasses `tuple[Item]`, but we'll also use `list[Item]` to represent an Item Set, depending on the mutability we need.\n",
    "\n",
    "We also make a function `make_item_set`, but this function is actually not used in our final parser. It is just to show what an Item Set made from a single rule looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Item(base=Rule: NTerm(Expr) <= NTerm(Expr) Term('+') NTerm(Atom), dot=0),\n",
      " Item(base=Rule: NTerm(Expr) <= NTerm(Expr) Term('+') NTerm(Atom), dot=1),\n",
      " Item(base=Rule: NTerm(Expr) <= NTerm(Expr) Term('+') NTerm(Atom), dot=2),\n",
      " Item(base=Rule: NTerm(Expr) <= NTerm(Expr) Term('+') NTerm(Atom), dot=3))\n"
     ]
    }
   ],
   "source": [
    "class Item(NamedTuple):\n",
    "    base: Rule\n",
    "    dot: int\n",
    "\n",
    "    def move_right(self) -> Self:\n",
    "        return Item(base=self.base, dot=self.dot + 1)\n",
    "\n",
    "    def right_of_dot(self) -> Optional[Token]:\n",
    "        if self.dot < len(self.base.right):\n",
    "            return self.base.right[self.dot]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "class ItemSet(tuple[Item]):\n",
    "    pass\n",
    "\n",
    "\n",
    "def make_item_set(rule: Rule) -> ItemSet:\n",
    "    items = (Item(base=rule, dot=dot) for dot in range(len(rule.right) + 1))\n",
    "    return ItemSet(items)\n",
    "\n",
    "\n",
    "pprint(make_item_set(make_rule(\"Expr <= Expr '+' Atom\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closure and State\n",
    "\n",
    "Item Set is an important concept in LR parsers, because it represents a **State** that our parser is in, if you imagine our parser as a _finite automaton_. But not all Item Sets represent a possible and reachable State in our parsing process. The Item Sets that do represent a State are called a closed Item Set, or a **Closure**.\n",
    "\n",
    "And thus, we can equte a State to a Closure, or a closed Item Set. We'll mostly be using the word State from here, but just know that these terms are essentially equivalent.\n",
    "\n",
    "From any Item Set, we can _close_ it by the following steps:\n",
    "1. Gather all the Non-Terminals that are immediately after a \"dot\" from all Items in the Item Set.\n",
    "1. For all rules starting with these Non-Terminals, add an Item to our Item Set, with \"dot\" all set at 0.\n",
    "1. Repeat until there is no new Items to be added.\n",
    "\n",
    "For example, if Non-Terminal `X` is after a dot in the existing set, and there is a rule `X <= Y Z`, then add `X <= ^ Y Z` to the set.\n",
    "\n",
    "The Python implementation of this might take some time to understand, so please take your time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(ItemSet):\n",
    "    pass\n",
    "\n",
    "\n",
    "def make_closure(rules: tuple[Rule], items: list[Item]) -> State:\n",
    "    non_terminals = deque(\n",
    "        filter(\n",
    "            lambda token: isinstance(token, NTerm),\n",
    "            [item.right_of_dot() for item in items],\n",
    "        )\n",
    "    )\n",
    "    seen = set()\n",
    "    while non_terminals:\n",
    "        non_term = non_terminals.popleft()\n",
    "        if non_term not in seen:\n",
    "            for rule in rules:\n",
    "                if rule.left == non_term:\n",
    "                    new_item = Item(base=rule, dot=0)\n",
    "                    if isinstance(new_item.right_of_dot(), NTerm):\n",
    "                        non_terminals.append(new_item.right_of_dot())\n",
    "                    items.append(new_item)\n",
    "        seen.add(non_term)\n",
    "\n",
    "    return State(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Table"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
